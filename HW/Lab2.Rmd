---
title: "Lab 2: Sentiment Analysis I"
author: "Sofia Ingersoll"
date: "2024-04-10"
output: html_document
---
## Assignment (Due 4/16 by 11:59 PM)

### Obtain your data and load it into R

-   Access the Nexis Uni database through the UCSB library: <https://www.library.ucsb.edu/research/db/211>

-   Choose a key search term or terms to define a set of articles.

-   Use your search term along with appropriate filters to obtain and download a batch of at least 100 full text search results (.docx). You are limited to downloading 100 articles at a time, so if you have more results than that, you have to download them in batches (rows 1-100, 101-200, 201-300 etc.)

    Guidance for {LexisNexisTools} : <https://github.com/JBGruber/LexisNexisTools/wiki/Downloading-Files-From-Nexis>

-   Read your Nexis article documents into RStudio.

-   Use the full text of the articles for the analysis. Inspect the data (in particular the full-text article data).

-   If necessary, clean any artifacts of the data collection process (hint: this type of thing should be removed: "Apr 04, 2022( Biofuels Digest: <http://www.biofuelsdigest.com/Delivered> by Newstex") and any other urls)

-   Remove any clear duplicate articles. LNT has a method for this, but it doesn't seem to work, so you probably need to do it manually.

### Explore your data and conduct the following analyses:

```{r setup, include=FALSE}
# Set the working directory to the folder containing the .docx files
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(LexisNexisTools)
library(tidyverse)
library(tidytext)
library(textdata)
library(stringr)
library(dplyr)
library(readr)
library(here)
library(gt)


knitr::opts_knit$set(root.dir = here("Nexis", "tiktokban"))
```


### Load Articles 
```{r load_articles}
# List all .docx files in the directory
post_files <- list.files(pattern = ".docx",
                         full.names = TRUE,
                         recursive = TRUE, 
                         ignore.case = TRUE)

# Print the first few file paths to verify
#head(post_files) 
```

```{r lnt_object}
dat <- lnt_read(post_files)

# pull out respective table slots
meta_df <- dat@meta
articles_df <- dat@articles
paragraphs_df <- dat@paragraphs

dat2 <- tibble(
 id = articles_df$ID,
 date = meta_df$Date,
 headline = meta_df$Headline,
 text = articles_df$Article
)
```
### Article Inspection & Wrangling 

The date is not being read in properly, so we're going to wrangle the lnt object. Upon reflection of the documents I've downloaded, they all contain unique date patterns as a result of the different document types. Some examples include:

`Summer, 2012`
`September 24, 2021`
`January/February, 2023`
`2021`
`November, 2022`

Here we see that the dates are being stored in `Newspaper` column in an usual pattern.

```{r inspect_meta_data}
head(meta_df,3) %>% 
  gt()
```

```{r date_correction}
# Extract year from the date column using parse_date_time
meta_df$year <- year(parse_date_time(meta_df$Newspaper, orders = c("mdy", "dmy", "ymd", "my", "dm", "md", "y", "m", "d")))


# pull date info from true column & store year 
dat2 <- tibble(
  id = articles_df$ID,
  date = meta_df$Newspaper,
  headline = meta_df$Headline,
  text = articles_df$Article,
  year = meta_df$year
)

unique(dat2$year)
```

```{r inspect_articles}
# this is the easiest format to scan the first few lines of each article in Article
head(dat2, 1) %>% 
  gt()

# make sure we don't have any duplicate articles
#length(unique(articles_df$Article))
```

I tried using regex to remove the word "INTRODUCTION" from the beginning of text entries, but couldn't for now.

```{r clean_articles, eval=FALSE}
# this chunk was unsuccessful, leaving for now
# Clean up the 'headline' and 'text' columns in dat2
cleaned_dat2 <- dat2 %>%
  # Remove starting words from 'text' column
 # mutate(text = gsub("^\\s*(INTRODUCTION):?\\s*", "", text, ignore.case = TRUE)) 
 # mutate(text = gsub("^\\s*(INTRODUCTION):?\\s*", "", text, ignore.case = TRUE))
  mutate(text = gsub("^\\s*(INTRODUCTION):?\\s*", "", text, ignore.case = TRUE))


head(cleaned_dat2, 1) %>% 
  gt()
```

1.  Calculate mean sentiment across all your articles
```{r get_bing}
#load the bing sentiment lexicon from tidytext
bing_sent <-  get_sentiments("bing")
```

```{r text_words}
text_words <- dat2 %>% 
  unnest_tokens(output = word,
                # from the datafrane
                input = text,
                token = "words"
                )

#text_words

#Let's start with a simple numerical score
sent_words <- text_words %>%  
  # remove stopwords at token level
  anti_join(stop_words, by ="word") %>% 
  # apply scorees
  inner_join(bing_sent, by = "word") %>% 
  # create numerical variable
  mutate(sent_num = case_when(
    sentiment == "negative" ~ -1,
    sentiment == "positive" ~ 1
  )
  )
         
head(sent_words, 6) %>% 
  gt()
```

```{r mean_sent}
# overall sentiment
sent_article <- sent_words %>% 
  group_by(headline) %>% 
  count(id, sentiment) %>% 
  pivot_wider(names_from = sentiment,
             values_from = n) %>% 
  mutate(polarity = positive-negative) 

mean(sent_article$polarity, na.rm = TRUE)
  

# individual article scores
sent_articles <- sent_words %>% 
  group_by(headline) %>%  
  summarize(avg_sentiment = mean(sent_num))
  
  
head(sent_articles, 6) %>% 
  gt()
```


```{r plot_sent_scores, fig.height=4, fig.width=8}
ggplot(sent_article, aes(id)) +
  
  geom_col(aes(y = negative), 
           stat = 'identity',
           fill = "purple4",
           alpha = 0.8) +
  
  geom_col(aes(y = positive), 
           stat = 'identity',
           fill = "hotpink",
           alpha = 0.5) +
  
  labs(title = "Sentitment Analysis: Tik Tok Ban",
       subtitle = "April 16, 2024",
       y = " ") +
  
  theme_classic() +
  theme(
    plot.title = element_text(hjust = 0.5,
                              size = 20),
    
    plot.subtitle = element_text(hjust = 0.5,
                              size = 15),
    
    axis.title.x = element_text(hjust = 0.5)
  )
```

3.  Most common nrc emotion words and plot by emotion
```{r nrc_sentiment}
nrc_sent <- get_sentiments('nrc')

nrc_words_count <- text_words %>% 
  anti_join(stop_words, by = "word") %>% 
  inner_join(nrc_sent) %>%
  count(word, sentiment, sort = TRUE)

head(nrc_words_count, 6) %>% 
  gt()
```

```{r sent_counts, fig.height=4, fig.width=8}
sent_counts <- nrc_words_count %>% 
  group_by(sentiment) %>% 
  slice_max(n, n = 5) %>% 
  ungroup() %>% 
  # order from most to least frequent 
  mutate(word = reorder(word, n))
  
ggplot(data = sent_counts,
       aes(x = n,
           y = word,
           fill = sentiment)) +
  
  geom_col(show.legend = FALSE) +
  
  facet_wrap(~sentiment, scales = "free_y") +
  
  labs(title = "Sentitment Analysis: Tik Tok Ban",
       subtitle = "April 16, 2024",
       x = "Contribution to Sentiment",
       y = NULL) +
  
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5,
                              size = 20),
    
    plot.subtitle = element_text(hjust = 0.5,
                              size = 15),
    
    axis.title.x = element_text(hjust = 0.5)
  )
  
#plot sent_counts  
sent_counts
```
4.  Look at the nrc contribution to emotion by word plots. Identify and reclassify or remove at least one term that gives misleading results in your context.

We're getting rid of `sex` because I don't feel like it's super imperative in the context of tik tok ban sentiments `joy` and `anticpation`. 
```{r sent_count_cleaned, fig.height=4, fig.width=8}
# Alternatively, remove misleading words altogether
# List of words to remove from the analysis
words_to_remove <- c("sex")

# Remove rows containing the misleading words
sent_counts <- sent_counts[!sent_counts$word %in% words_to_remove, ]


# Create bar plot
ggplot(sent_counts, aes(x = n, y = word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = "Sentiment Analysis: Tik Tok Ban",
       subtitle = "April 16, 2024",
       x = "Contribution to Sentiment",
       y = NULL) +
  theme_bw()+
  theme(
    plot.title = element_text(hjust = 0.5,
                              size = 20),
    
    plot.subtitle = element_text(hjust = 0.5,
                              size = 15),
    
    axis.title.x = element_text(hjust = 0.5)
  )
```

5.  Plot the amount of nrc emotion words as a percentage of all the emotion words used each day (aggregate text from articles published on the same day). How does the distribution of emotion words change over time? Can you think of any reason this would be the case?

So this is actually impossible for the data that I'm working with. To follow the request as closely as possible, I am aggregating by year and removing the years that fall outside of expected publication dates.

As expected, a drastic influx of NRC words appeared during the years that tik tok become more prevelent and controversial in American media. I am surprised by the decrease in NRC word frequency in the most recent year because there are politicians currently at work trying to actively ban tik tok. I haven't been able to keep super up to date, but if I rememeber correctly, there was something on their ballot for potentially banning tik tok in the next several months. I would expect more articles to include NRC because of this, but this could be a result of the distribution of years I've downloaded. The majority appear to be from the spiked years, so this is likely influencing the plot below and skewing the spike compared to 2023-2024.
```{r agg_nrc_perc_plot}
# Aggregate text from articles published on the same year
dat2_grouped <- dat2 %>%
  # Remove years 1986 and 1994 because they're not related to today's tik tok
  filter(year != 1986 & year != 1994) %>%  
  na.omit(year) %>% 
  group_by(year) %>%
  summarise(agg_text = paste(text, collapse = " "))

# Tokenize the aggregated text
text_words <- dat2_grouped %>% 
  unnest_tokens(output = word,
                input = agg_text,
                token = "words")

# Join with NRC sentiment lexicon
nrc_words_count <- text_words %>% 
  anti_join(stop_words, by = "word") %>% 
  inner_join(get_sentiments("nrc")) %>%
  count(year, sentiment, sort = TRUE)

# Calculate the percentage of NRC emotion words out of all emotion words used each year
nrc_percentage <- nrc_words_count %>%
  group_by(year) %>%
  summarise(nrc_count = sum(n)) %>%
  left_join(
    dat2_grouped %>% select(year),
    by = "year"
  ) %>%
  mutate(total_count = sum(nrc_count)) %>%
  mutate(percentage = (nrc_count / total_count) * 100)

# Plot the distribution of NRC emotion word percentage over time
ggplot(nrc_percentage, aes(x = year, y = percentage)) +
  geom_line(col = 'skyblue',
            size = 3) +
  labs(title = "Percentage of NRC Emotion Words Over Years",
       subtitle = "Topic: Tik Tok Ban",
       x = "Year",
       y = "Percentage of NRC Emotion Words") +
  theme_classic() +
  theme(
    plot.title = element_text(hjust = 0.5,
                              size = 20),
    
    plot.subtitle = element_text(hjust = 0.5,
                              size = 15)
  )
```

