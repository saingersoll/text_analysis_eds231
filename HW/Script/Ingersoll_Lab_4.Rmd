---
title: "Lab 4"
author: "Sofia Ingersoll"
date: "2024-05-07"
output: html_document
---

Lab 4 Assignment: Due May 7 at 11:59pm

```{r set_up, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# reproducibility
set.seed(1234)

# load libraries
# naive-bayes
library(vip)
library(discrim) 
library(tidytext)
library(tidyverse)
library(tidymodels)
library(textrecipes)


library(doParallel)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops
library(ranger)
library(parsnip)
library(yardstick)

# load data
urlfile ="https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv"
incidents_df<-readr::read_csv(url(urlfile),
                              show_col_types = FALSE)
```

```{r split-data}
incidents2class <- incidents_df %>%
  mutate(fatal = factor(if_else(
                        is.na(Deadly),
                        "non-fatal", "fatal")))


incidents_split <- initial_split(incidents2class,
                                 prop = 0.8,
                                 strata = fatal)

incidents_train <- training(incidents_split)
incidents_test <- testing(incidents_split)
```

```{r recipe}
incidents_rec <- recipe(fatal ~ Text, data = incidents_train)
```

```{r pre-process}
recipe <- incidents_rec %>%
  step_tokenize(Text) %>%
  # max 5000 is perfect performance
  step_tokenfilter(Text, max_tokens = 1000) %>%
  step_tfidf(Text)

# cross validation folds
incidents_folds <- vfold_cv(incidents_train,
                            v = 5)
```

1. Select another classification algorithm.  [Random Forest]
2. Conduct an initial out-of-the-box model fit on the training data and prediction on the test data.  Assess the performance of this initial model. 

The performance of the out-of-the-box model displays bias towards non-fatal articles. However, it was able to successfully identify 151 fatal text.

```{r rf_box_model}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                       Tuning Random Forest                           ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ensemble of decision trees
# mtry subsets at each parameter to create tree (must be n-1 no. predictors)
# trees is no. of tree models 
rf_model <- rand_forest(trees = 500) %>% 
  # this is a package loaded in libs
  set_engine('ranger') %>% 
  set_mode('classification')

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                           Workflow                                   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# establish wkflw for model
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(recipe)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                             Fit Model                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Fit the model
train_fit_rf <- fit_resamples(
  rf_workflow, 
  incidents_folds, 
  control = control_resamples(save_pred = TRUE)
)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                       Collect Metrics                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
rf_metrics <- collect_metrics(train_fit_rf, metric = "roc_auc")

rf_predictions <- collect_predictions(train_fit_rf, metric = "roc_auc")

rf_predictions
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                        Visual of Fit Model                          ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# visualize model
# create confusion matrix
rf_cm <- rf_predictions %>% 
  conf_mat(truth = fatal,
           estimate = .pred_class) %>% 
  # plot confusion matrix with heatmap
  autoplot(type = "heatmap") + 
  # change theme
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 30,
                                   hjust=1)) +
  #rotate axis labels
  labs(title = "Random Forest")


rf_predictions %>%
  group_by(id) %>%
  roc_curve(truth = fatal, .pred_fatal) %>%
  autoplot() +
  labs(title = "ROC curve for Climbing Incident Reports")

rf_cm
```

3. Select the relevant hyperparameters for your algorithm and tune your model.

4. Conduct a model fit using your newly tuned model specification.  How does it compare to your out-of-the-box model?

There are fewer mistakes in the tuned model, compared to the out-of-the-box model. However, the difference is minimal and demonstrates little improvement overall,  it was able to successfully identify 155 fatal text.  Still significant bias for non-fdtal text and mislabeling a lot.

```{r tuned_rf_model}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                       Tuning Random Forest                           ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ensemble of decision trees
# mtry subsets at each parameter to create tree (must be n-1 no. predictors)
# trees is no. of tree models 
rf_tune_model <- rand_forest(trees = tune()) %>% 
  # this is a package loaded in libs
  set_engine('ranger') %>% 
  set_mode('classification')

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                           Workflow                                   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# establish wkflw for model
rf_tune_workflow <- workflow() %>% 
  add_model(rf_tune_model) %>% 
  add_recipe(recipe)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                        Cross Validation                              ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# use cross validation to tune mtry and treesfor 5 parameters combinations
# this takes a very long time to run
rf_cv_tune = rf_tune_workflow %>%
  tune_grid(resamples = incidents_folds,
            grid = 5) 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Finalize Workflow                            ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# store the best tuning metrics
rf_tune_best <- show_best(rf_cv_tune, n = 1, metric = "roc_auc")

# finalize workflow
rf_tune_final <-  finalize_workflow(rf_tune_workflow,
                                    rf_tune_best)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                             Fit Model                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# similar functions here.
# Fit the model to resampled data
train_fit_rf_tune <- fit_resamples(rf_tune_final,
                                   incidents_folds, 
                                   control = control_resamples(save_pred = TRUE))

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                       Collect Metrics                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Collect metrics
rf_tune_metrics <- collect_metrics(train_fit_rf_tune, metric = "roc_auc")

# Get predictions on the test set
rf_tune_predictions <- collect_predictions(train_fit_rf_tune, new_data = incidents_test)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                        Visual of Fit Model                          ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# visualize model
# create confusion matrix
rf_tune_cm <- rf_tune_predictions %>% 
  conf_mat(truth = fatal,
           estimate = .pred_class) %>% 
  # plot confusion matrix with heatmap
  autoplot(type = "heatmap") + 
  # change theme
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 30,
                                   hjust=1)) +
  #rotate axis labels
  labs(title = "Random Forest")


rf_tune_predictions %>%
  group_by(id) %>%
  roc_curve(truth = fatal, .pred_fatal) %>%
  autoplot() +
  labs(title = "ROC curve for Climbing Incident Reports")

rf_tune_cm
```

5.
  a. Use variable importance to determine the terms most highly associated with non-fatal reports?  What about terms associated with fatal reports? OR
  b. If you aren't able to get at variable importance with your selected algorithm, instead tell me how you might in theory be able to do it. Or how you might determine the important distinguishing words in some other way. 
  
My attempts to use variable importance were not fruitful, but I've left my attempts. Random Forests ranger does not provid eModel-specific variable importance scores. In the future, I would use a model like xgboost that provides the scores.
  
  
```{r variable_importance, eval = FALSE}
# Visualize fit model
# Error: Model-specific variable importance scores are currently not available for this type of model.
# this worked when i was trying to use xgboost while practicing
train_fit_rf_tune %>% 
  vip(geom = 'col',
      aesthetics = list(fill = 'midnightblue')) 


# Extract variable importance scores
importance_scores <- best_model$.fit$variable.importance

# Get variable names
variable_names <- names(importance_scores)

# Create a data frame to store variable names and importance scores
variable_importance <- data.frame(
  variable = variable_names,
  importance = importance_scores
)

# Sort variables by importance for non-fatal reports
non_fatal_importance <- variable_importance %>%
  arrange(desc(importance))

# Filter terms associated with non-fatal reports
terms_non_fatal <- non_fatal_importance$variable[1:10]  # Adjust the number of terms as needed

# Sort variables by importance for fatal reports
fatal_importance <- variable_importance %>%
  arrange(importance)

# Filter terms associated with fatal reports
terms_fatal <- fatal_importance$variable[1:10]  # Adjust the number of terms as needed

# Display the top terms associated with non-fatal and fatal reports
print("Top terms associated with non-fatal reports:")
print(terms_non_fatal)

print("Top terms associated with fatal reports:")
print(terms_fatal)
```

6. Predict fatality of the reports in the test set.  Compare this prediction performance to that of the Naive Bayes and Lasso models.  Why do you think your model performed as it did, relative to the other two?

NB model performed the worst, followed by the RF models, leaving the Lasso model in the lead. This is likely because Lasso performs feature selection by shrinking some coefficients to zero, effectively selecting only the most relevant features.

```{r naive_bayes_model}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ---        NB Spec.    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ---        NB Wkflw    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nb_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(nb_spec)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ---       fit model    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nb_rs <- fit_resamples(
  nb_wf, 
  incidents_folds, 
  control = control_resamples(save_pred = T)
)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# --- collect metrics    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)

nb_rs_predictions

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ---       visualize    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = fatal, .pred_fatal) %>%
  autoplot() +
  labs(title = "ROC curve for Climbing Incident Reports")


conf_mat_resampled(nb_rs, tidy = F) %>%
  autoplot(type = "heatmap")
```

```{r lasso_model}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# --- Penalty Tuning Spec----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
tune_spec <- logistic_reg(penalty = tune(),
                          # mixture used for feature selection
                          # tells model to use L1 penalty term, this drive all the way to 0
                          # L2 would only do feature selection close to zero
                          mixture = 1) %>% 
  set_mode('classification') %>% 
  set_engine('glmnet')

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# --- Create Lambda Grid ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
lambda_grid <- grid_regular(penalty(),
                            # try this many values for hyperparam
                            levels = 30)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----     Tune Wkflw.   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
tune_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(tune_spec)

# run model 300 times (30 params x 10 folds)
tunes_rs <- tune_grid(
  tune_wf,
  incidents_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = T)
)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----   Fit Resamples   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#lasso_rs <- fit_resamples(
 # lasso_wf,
  #incidents_folds,
  #control = control_resamples(save_pred = TRUE))

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ---- Pentalty Show Best----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
tune_best_roc <- tunes_rs %>% 
  select_best("roc_auc")


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ---- Finalize Workflow ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
final_lasso <- finalize_workflow(tune_wf,
                                 tune_best_roc)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ---- final fitmetrics  ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
lasso_fit_final <- fit_resamples(final_lasso,
                                 incidents_folds,
                                 control = control_resamples(save_pred = TRUE)) 


final_lasso <- collect_metrics(lasso_fit_final)

# Extract predictions from the fitted resamples
lasso_predictions <- lasso_fit_final %>% 
  collect_predictions()


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ---       visualize    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
conf_mat_resampled(lasso_fit_final, tidy = F) %>%
  autoplot(type = "heatmap")


lasso_predictions %>%
  group_by(id) %>%
  roc_curve(truth = fatal, .pred_fatal) %>%
  autoplot() +
  labs(title = "ROC curve for Climbing Incident Reports")
```

