---
title: "Lab5_Demo"
author: "Mateo Robbins"
date: "2024-05-08"
output: html_document
---

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(widyr)         # for co-occurance counting
library(irlba)
library(broom) 
library(textdata)
library(ggplot2)
library(dplyr)
```

#### Word Embeddings

We'll start off today by loading the climbing incident data again. This week we aren't interested in the fatality variable, just the text of the reports.

```{r data,}
incidents_df <- read_csv("https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv",
                       show_col_types = FALSE)
```

First, let's calculate the unigram probabilities -- how often we see each word in this corpus.

Introduction into advance sematic anaylsis and language modeling

```{r unigrams}
# let's calculate the frequency of 
unigram_probs <- incidents_df %>% 
  unnest_tokens(word, Text) %>% 
  anti_join(stop_words, by = 'word') %>% 
  count(word, sort = TRUE) %>% 
  mutate(p = n/sum(n))
  
unigram_probs
```

OK, so that tells us the probability of each word.

Next, we need to know how often we find each word near each other word -- the skipgram probabilities. In this case we'll define the word context as a five-word window. We'll slide that window across all of our text and record which words occur together within that window.

We'll add an ngramID column that contains constituent information about each 5-gram we constructed by sliding our window.

Using context to understand which words go together

```{r make-skipgrams}
skipgrams <- incidents_df %>% 
  # window of 5 word sequences using this function
  unnest_tokens(ngram, Text, toke = "ngrams", n = 5) %>% 
  mutate(ngramID = row_number()) %>% 
  tidyr::unite(skipgramID, ID, ngramID) %>% 
  # let's take a look at our output
  unnest_tokens(word, ngram) %>% 
  anti_join(stop_words)
  
skipgrams
```

Now we use widyr::pairwise_count() to sum the total # of occurrences of each pair of words.

```{r pairwise_count}
skipgram_probs  <- skipgrams %>% 
  # count each pair of words in each skipID appears
  # this is helping us identify context ques
  widyr::pairwise_count(item = word, 
                        feature = skipgramID,
                        # Reduces data/ simplifies
                        diag = FALSE,
                        # this removes the duplicates 
                        upper = FALSE) %>% 
  # what is the probability of a given pair
  # are within 5 words of each other in all of these articles
  mutate(p = n/sum(n)) %>% 
  # this shows us we're getting duplicate responses for flipped item1 and 2 occurances
  arrange(-p)


skipgram_probs
```

The next step is to normalize these probabilities, that is, to calculate how often words occur together within a window, relative to their total occurrences in the data. We'll also harmnoize the naming conventions from the different functions we used.

```{r norm-prob}
normalized_probs <- skipgram_probs %>% 
  rename(word1 = item1,
         word2 = item2) %>% 
  # combine unigram probabilities
  # and 
  left_join(unigram_probs %>% 
              select(word1 = word, 
                     p1 = p),
                     by = 'word1'
  ) %>% 
  left_join(unigram_probs %>% 
              select(word2 = word, 
                     p2 = p),
            by = 'word2'
            ) %>% 
  mutate(p_together = p/p1/p2)
  
normalized_probs[1:10, ]
```

Now we have all the pieces to calculate the point-wise mutual information (PMI) measure. It's the logarithm of the normalized probability of finding two words together. PMI tells us which words occur together more often than expected based on how often they occurred on their own.

Then we cast to a matrix so we can use matrix factorization and reduce the dimensionality of the data.

```{r pmi}
# co-occurance matrix
pmi_matrix <- normalized_probs %>% 
  # log of prop of 2 words occuring together
  mutate(pmi = log10(p_together)) %>% 
  # we know what pair each calculated value corresponds to
  cast_sparse(word1, word2, pmi)

# too many! we want 100 dims
dim(pmi_matrix)
```

We do the singular value decomposition with irlba::irlba(). It's a "partial decomposition" as we are specifying a limited number of dimensions, in this case 100.

R version 4.3.1
matrix 1.6-4

```{r svd}
# reference all non-zero elements of matric
# replace all NA as zero
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

pmi_matrix
# decomposition guessing / checking iteration to continually improve solution
# rather than simply calculating
# this can take a really long time with big data sets
# in those cases, you may want to set the number of iterations here
# this didn't fix my prob
remotes::install_version("Matrix", version = "1.6-4")


remotes::install_version("Matrix", version = "1.7-0")
library(Matrix)
install.packages("Matrix", dependencies = TRUE)


pmi_svd <- irlba::irlba(pmi_matrix, 100, verbose = FALSE)

word_vectors <- pmi_svd

rownames(word_vectors) <- rownames(pmi_matrix)
```

These vectors in the "u" matrix are contain "left singular values". They are orthogonal vectors that create a 100-dimensional semantic space where we can locate each word. The distance between words in this space gives an estimate of their semantic similarity.

```{r syn_function}
# all vectors, word of choice, og word to remove similarities later
search_synonyms <- function(word_vectors,  selected_vector, original_word) {
  
  # define data
  dat <- word_vectors %>% 
  # compare two vectors using dot product
    selected_vector
  
  similarities <- as.data.frame(dat) %>% 
    tibble(token = rownames(dat), similarity = dat[,1]) %>% 
    # remove og word so it isn't returned as similarity
    filter(token != original_word) %>% 
    arrage(-similarity) %>% 
    select(token, similarity)
  
  return(similarities)
}
```

Let's test it out!

```{r find-synonyms}
fall <- search_synonyms(word_vectors, word_vectors["fall"], "fall")


slip <- search_synonyms(word_vectors, word_vectors["slip"], "slip")


ice <- search_synonyms(word_vectors, word_vectors["ice"], "ice")


snow <- search_synonyms(word_vectors, word_vectors["snow"], "snow")


danger <- search_synonyms(word_vectors, word_vectors["danger"], "danger")
```

Here's a plot for visualizing the most similar words to a given target word.

```{r plot-synonyms}
slip %>% 
  mutate(selected = "slip") %>% 
  bind_rows(fall %>% 
              mutate(
                selected = "fall"
              )) %>% 
  group_by(by = selected) %>% 
  top_n(15, similarity) %>% 
  mutate(token = reorder(token, similarity)) %>% 
  ggplot(aes(token = reorder(token, similarity, fill = selected))) +
  geom_col(show_legend = FALSE) %>% 
  facet_wrap(~selected, scales = "free") +
  coord_flip()
```

One of the cool things about representing words as numerical vectors is that we can use math on those numbers that has some semantic meaning.

```{r word-math}
# 
snow_danger <- word_vectors["snow"] + word_vectors["danger"]

# which words have similar vectors as the resulting addition vector above?
search_synonyms(word_vectors, snow_danger, "")


# remove snow and see what is associated with danger now
no_snow <- word_vectors["danger"] - word_vectors["snow"]

# which words have similar vectors as the resulting addition vector above?
search_synonyms(word_vectors, no_snow, "")
```
